{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from scipy.optimize import minimize\n",
    "import pandas_datareader as web\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import h5py\n",
    "import yaml\n",
    "from ticker_download_manager import TickerDownloadManager\n",
    "from date_manager import DateManager\n",
    "from ticker_predict_upload import TickerPredictUpload\n",
    "from s3_uploader import S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = TickerDownloadManager(os.path.join(\"input\", \"annual\"))\n",
    "dm = DateManager()\n",
    "tpu = TickerPredictUpload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the past year of ticker close prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df, start_date, end_date = tdm.get_latest_tickers(days_in_past=252, use_cache=True)\n",
    "print(f\"{start_date} to {end_date}\")\n",
    "long_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot the close prices for better analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df = tpu.pivot_ticker_close_wide(long_df)\n",
    "date_from = wide_df.index[0]\n",
    "date_to = wide_df.index[-1]\n",
    "wide_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate % change and covert to a percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df = wide_df.pct_change()\n",
    "returns_df = returns_df.iloc[1:] * 100\n",
    "returns_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_returns = returns_df.mean()\n",
    "mean_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = returns_df.cov()\n",
    "cov_np = cov.to_numpy()\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate 10,000 portfolios by generating random weights\n",
    "\n",
    "Softmax is used to generate random vector of positive floats summing to 1.0. Not used if short selling is allowed in the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_random_distribution(D):\n",
    "    \"\"\"\n",
    "    Generates a NumPy array of D random floats that sum to 1.0 using the softmax function.\n",
    "\n",
    "    The process involves:\n",
    "    1. Generating D random numbers (from a standard normal distribution).\n",
    "    2. Applying the softmax function to these numbers. Softmax converts a vector\n",
    "        of numbers into a probability distribution where each element is non-negative\n",
    "        and all elements sum to 1.0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    D : int \n",
    "        The desired number of elements (dimension) in the output array.\n",
    "        Must be a positive integer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A NumPy array of shape (D,) containing floats that sum to 1.0.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError: If D is not a positive integer.\n",
    "    \"\"\"\n",
    "    if not isinstance(D, int) or D <= 0:\n",
    "        raise ValueError(\"Dimension D must be a positive integer.\")\n",
    "    random_inputs = np.random.randn(D)\n",
    "    stable_inputs = random_inputs - np.max(random_inputs)\n",
    "    exponentials = np.exp(stable_inputs)\n",
    "    softmax_output = exponentials / np.sum(exponentials)\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_portfolios = 10_000\n",
    "simulated_returns = np.zeros(n_portfolios)\n",
    "simulated_risks = np.zeros(n_portfolios)\n",
    "random_weights = []\n",
    "rand_range = 1.0\n",
    "\n",
    "for i in range(n_portfolios):\n",
    "    D = len(tdm.tickers)\n",
    "    # w = np.random.random(D) * rand_range - rand_range / 2  # Allows short selling\n",
    "    # w[-1] = 1 - w[:-1].sum()\n",
    "    # np.random.shuffle(w)\n",
    "    w = softmax_random_distribution(D)  # No short selling\n",
    "    random_weights.append(w)\n",
    "    simulated_return = mean_returns.dot(w)\n",
    "    simulated_risk = np.sqrt(w.dot(cov_np).dot(w))\n",
    "    simulated_returns[i] = simulated_return\n",
    "    simulated_risks[i] = simulated_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate minimum variance portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_bounds = [(-0.5, None)] * D  # Allows shorting\n",
    "# weight_bounds = [(0.0, 1.0) for _ in range(D)]  # No shorting, no leverage\n",
    "weight_bounds = [(0.0, 4.0 / D) for _ in range(D)]  # Limit how much can be invested in one asset, no shorting, no leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portfolio_variance(weights):\n",
    "    return weights.dot(cov_np).dot(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_weights_constraint(weights):\n",
    "    return weights.sum() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_var_result = minimize(\n",
    "    fun=get_portfolio_variance,\n",
    "    x0=np.ones(D) / D,\n",
    "    method=\"SLSQP\",\n",
    "    bounds=weight_bounds,\n",
    "    constraints={\"type\": \"eq\", \"fun\": portfolio_weights_constraint},\n",
    ")\n",
    "min_var_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_var_risk = np.sqrt(min_var_result.fun)\n",
    "min_var_weights = min_var_result.x\n",
    "min_var_return = min_var_weights.dot(mean_returns)\n",
    "min_var_risk, min_var_weights, min_var_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate efficient frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_portfolios = 100\n",
    "max_simulated_return = max(simulated_returns)\n",
    "print(f\"Possible returns range: {min_var_return:.4f} to {max_simulated_return:.4f}\")\n",
    "target_returns = np.linspace(min_var_return, max_simulated_return, num_portfolios)\n",
    "target_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_returns_constraint(weights, target_return):\n",
    "    return weights.dot(mean_returns) - target_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = [\n",
    "    {\"type\": \"eq\", \"fun\": target_returns_constraint, \"args\": [target_returns[0]]},\n",
    "    {\"type\": \"eq\", \"fun\": portfolio_weights_constraint},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weight bounds\", weight_bounds)\n",
    "\n",
    "optimized_risks = []\n",
    "for target_return in target_returns:\n",
    "    constraints[0][\"args\"] = [target_return]\n",
    "    result = minimize(\n",
    "        fun=get_portfolio_variance,\n",
    "        x0=np.ones(D) / D,\n",
    "        method=\"SLSQP\",\n",
    "        bounds=weight_bounds,\n",
    "        constraints=constraints,\n",
    "    )\n",
    "    if result.status == 0:\n",
    "        optimized_risks.append(np.sqrt(result.fun))\n",
    "    else:\n",
    "        optimized_risks.append(np.nan)\n",
    "        print(f\"Infeasible target return: {target_return:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sharpe Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date = dm.get_today_date()\n",
    "risk_free_rate_filename = os.path.join(\"input\", f\"Risk Free Rate {today_date}.json\")\n",
    "if os.path.exists(risk_free_rate_filename):\n",
    "    print(\"Reading risk-free rate cache...\")\n",
    "    with open(risk_free_rate_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        risk_free_rate_data = json.load(f)\n",
    "        print(risk_free_rate_data)\n",
    "        daily_risk_free_rate = risk_free_rate_data[\"daily_risk_free_rate\"]\n",
    "else:\n",
    "    end_date = datetime.datetime.now()\n",
    "    start_date = end_date - relativedelta(years=1)\n",
    "    print(start_date, end_date)\n",
    "    tb3m_df = web.DataReader(\"DTB3\", \"fred\", start_date, end_date).sort_values(\n",
    "        \"DATE\", ascending=False\n",
    "    )\n",
    "    risk_free_rate = float(tb3m_df.iloc[0][\"DTB3\"])\n",
    "    daily_risk_free_rate = risk_free_rate / 252\n",
    "    risk_free_rate_date = str(tb3m_df.index[0])\n",
    "    print(daily_risk_free_rate)\n",
    "    risk_free_rate_data = {\n",
    "        \"risk_free_rate\": risk_free_rate,\n",
    "        \"daily_risk_free_rate\": daily_risk_free_rate,\n",
    "        \"risk_free_rate_date\": risk_free_rate_date,\n",
    "    }\n",
    "    with open(risk_free_rate_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(risk_free_rate_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sharpe_ratio(weights):\n",
    "    mean = weights.dot(mean_returns)\n",
    "    risk = np.sqrt(weights.dot(cov_np).dot(weights))\n",
    "    return -(mean - daily_risk_free_rate) / risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratio_result = minimize(\n",
    "    fun=negative_sharpe_ratio,\n",
    "    x0=np.ones(D) / D,\n",
    "    method=\"SLSQP\",\n",
    "    bounds=weight_bounds,\n",
    "    constraints={\"type\": \"eq\", \"fun\": portfolio_weights_constraint},\n",
    ")\n",
    "sharpe_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sharpe_ratio = -sharpe_ratio_result.fun\n",
    "best_weights = sharpe_ratio_result.x\n",
    "opt_risk = np.sqrt(best_weights.dot(cov_np).dot(best_weights))\n",
    "opt_return = best_weights.dot(mean_returns)\n",
    "best_sharpe_ratio, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights_pct = pd.Series(best_weights * 100, index=mean_returns.index)\n",
    "best_weights_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights_pct_dict = best_weights_pct.to_dict()\n",
    "best_weights_pct_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate tangency line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tangency_max_risk = max(optimized_risks)\n",
    "tangency_xs = np.linspace(0, tangency_max_risk, 100)\n",
    "tangency_ys = daily_risk_free_rate + best_sharpe_ratio * tangency_xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINALLY! Put it all on a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "ax.plot(\n",
    "    optimized_risks, target_returns, c=\"#009E73\", zorder=1, label=\"Efficient Frontier\"\n",
    ")\n",
    "ax.plot(tangency_xs, tangency_ys, c=\"#F0E442\", zorder=1, label=\"Tangency line\")\n",
    "ax.scatter(\n",
    "    simulated_risks,\n",
    "    simulated_returns,\n",
    "    alpha=0.1,\n",
    "    s=2,\n",
    "    c=\"#0072B2\",\n",
    "    zorder=10,\n",
    "    label=\"Portfolios\",\n",
    ")\n",
    "ax.scatter(\n",
    "    [opt_risk],\n",
    "    [opt_return],\n",
    "    c=\"#CC79A7\",\n",
    "    marker=\"*\",\n",
    "    s=200,\n",
    "    zorder=10,\n",
    "    label=\"Max Sharpe Ratio Portfolio\",\n",
    ")\n",
    "ax.scatter(\n",
    "    [min_var_risk], [min_var_return], c=\"#E69F00\", zorder=10, label=\"Min Var Portfolio\"\n",
    ")\n",
    "ax.set_xlabel(\"Daily Risk (σ)\")\n",
    "ax.set_ylabel(\"Daily Returns (%)\")\n",
    "ax.set_title(\"Efficient Frontier\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annualize optimum return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annualized_optimum_return = ((1 + opt_return / 100) ** 252 - 1) * 100\n",
    "annualized_optimum_risk = opt_risk * np.sqrt(252)\n",
    "print(annualized_optimum_return, annualized_optimum_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write an HDF5 file with everything needed to regenerate plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_optimization_plot_data_filename = os.path.join(\n",
    "    \"output\", \"portfolio_optimization_plot_data.h5\"\n",
    ")\n",
    "with h5py.File(portfolio_optimization_plot_data_filename, \"w\") as hf:\n",
    "    efficient_frontier_group = hf.create_group(\"efficient_frontier\")\n",
    "    tangency_line_group = hf.create_group(\"tangency_line\")\n",
    "    simulated_portfolios_group = hf.create_group(\"simulated_portfolios\")\n",
    "    max_sharpe_ratio_group = hf.create_group(\"max_sharpe_ratio\")\n",
    "    min_var_portfolio_group = hf.create_group(\"min_var_portfolio\")\n",
    "    efficient_frontier_group.create_dataset(\"xs\", data=optimized_risks)\n",
    "    efficient_frontier_group.create_dataset(\"ys\", data=target_returns)\n",
    "    tangency_line_group.create_dataset(\"xs\", data=tangency_xs)\n",
    "    tangency_line_group.create_dataset(\"ys\", data=tangency_ys)\n",
    "    simulated_portfolios_group.create_dataset(\"xs\", data=simulated_risks)\n",
    "    simulated_portfolios_group.create_dataset(\"ys\", data=simulated_returns)\n",
    "    max_sharpe_ratio_group.create_dataset(\"xs\", data=[opt_risk])\n",
    "    max_sharpe_ratio_group.create_dataset(\"ys\", data=[opt_return])\n",
    "    min_var_portfolio_group.create_dataset(\"xs\", data=[min_var_risk])\n",
    "    min_var_portfolio_group.create_dataset(\"ys\", data=[min_var_return])\n",
    "print(f\"Saved {portfolio_optimization_plot_data_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "s3u = S3Uploader()\n",
    "\n",
    "space_name = os.getenv(\"PORTFOLIO_OPTIMIZATION_SPACE_NAME\")\n",
    "s3u.upload_file(\n",
    "    portfolio_optimization_plot_data_filename,\n",
    "    space_name,\n",
    "    \"portfolio_optimization_plot_data.h5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload metadata about the plot for the front end UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"date_updated\": {\n",
    "        \"date_from\": str(date_from.date()),\n",
    "        \"date_to\": str(date_to.date()),\n",
    "    },\n",
    "    \"tickers\": tdm.tickers,\n",
    "    \"risk_free_rate\": risk_free_rate_data,\n",
    "    \"optimum_portfolio\": {\n",
    "        \"annualized_return\": float(annualized_optimum_return),\n",
    "        \"risk\": float(annualized_optimum_risk),\n",
    "        \"weights\": best_weights_pct_dict\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_filename = os.path.join(\"output\", \"optimization_metadata.yml\")\n",
    "with open(metadata_filename, \"w\") as f:\n",
    "    yaml.dump(metadata, f, default_flow_style=False)\n",
    "\n",
    "s3u.upload_file(\n",
    "    metadata_filename,\n",
    "    space_name,\n",
    "    \"optimization_metadata.yml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuzzy-system-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
